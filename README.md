# Inference ONNX Model with ONNX Runtime
[link refer](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/Inference_GPT2_with_OnnxRuntime_on_CPU.ipynb)
* speed up 2x
* For CPU, optimized graph is slightly different: FastGelu is replaced by BiasGelu.
## Export the loaded model
```bash
python convert_onnx.py
```
## Inference ONNX Model
```bash
python bert_onnxruntime.py
```
## offline optimization
* sometime OnnxRuntime cannot be fully optimized:
    * new subgraph generated by new export tool and not covered by older version of OnnxRuntime
    * exported model uses dynamic axis, make harder for shape inference
    * some optimization is better to done offline. Like change input tensor type from float32 to float16 avoid Cast nodes to achieve better performance in V100 and T4 GPU
```bash
python experiment.py
```  

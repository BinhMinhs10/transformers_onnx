# Inference ONNX Model with ONNX Runtime
[link refer](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/Inference_GPT2_with_OnnxRuntime_on_CPU.ipynb)
* speed up 2x
## Export the loaded model

```bash
python convert_onnx.py
```
## Inference ONNX Model
```bash
python bert_onnxruntime.py
```
